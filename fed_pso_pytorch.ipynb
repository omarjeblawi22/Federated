{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_LPVOK1cJgy"
      },
      "source": [
        "## **Import necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qb7maCRCp5oG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import copy\n",
        "import time as time\n",
        "from torch.autograd import Variable\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL0LgZyicNeW"
      },
      "source": [
        "# **Define variables for optimizer function and data processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0mVJYvop-e7"
      },
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define variables for optimizer function and data processing\n",
        "NUMOFCLIENTS = 10\n",
        "SELECT_CLIENTS = 0.5\n",
        "EPOCHS = 5\n",
        "CLIENT_EPOCHS = 5\n",
        "BATCH_SIZE = 10\n",
        "DROP_RATE = 0\n",
        "\n",
        "# Model config\n",
        "LOSS = nn.CrossEntropyLoss()\n",
        "NUMOFCLASSES = 10\n",
        "lr = 0.0025\n",
        "OPTIMIZER = optim.SGD  # You can customize the optimizer later if needed\n",
        "\n",
        "# PSO Acc\n",
        "\n",
        "ACC = 0.5\n",
        "LOCAL_ACC = 0.2\n",
        "GLOBAL_ACC = 0.2\n",
        "\n",
        "PARAMS = {\n",
        "    'acc': 0.5,\n",
        "    'local_acc': 0.2,\n",
        "    'global_acc': 0.3,\n",
        "    # Add other parameters as needed\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8UIyKn1qf9r"
      },
      "source": [
        "## Writes the data into a csv file\n",
        "\n",
        "### Part of preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0aCutHpqV4-"
      },
      "outputs": [],
      "source": [
        "def write_csv(method_name, list):\n",
        "    file_name = '{name}_CIFAR10_randomDrop_{drop}%_output_C_{c}_LR_{lr}_CLI_{cli}_CLI_EPOCHS_{cli_epoch}_TOTAL_EPOCHS_{epochs}_BATCH_{batch}.csv'\n",
        "    file_name = file_name.format(folder=\"origin_drop\",drop=DROP_RATE, name=method_name, c=SELECT_CLIENTS, lr=lr, cli=NUMOFCLIENTS, cli_epoch=CLIENT_EPOCHS, epochs=EPOCHS, batch=BATCH_SIZE)\n",
        "    f = open(file_name, 'w', encoding='utf-8', newline='')\n",
        "    wr = csv.writer(f)\n",
        "\n",
        "    for l in list:\n",
        "        wr.writerow(l)\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hj5smrNcWf9"
      },
      "source": [
        "## **Continue data preprocessing**\n",
        "\n",
        "Prepare training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtxFjBVHqh-R"
      },
      "outputs": [],
      "source": [
        "def load_dataset():\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "    # Extract actual data from the data loaders\n",
        "    x_train, y_train = next(iter(DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True)))\n",
        "    x_test, y_test = next(iter(DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)))\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6gMBvDccXiX"
      },
      "source": [
        "## **Instantiate Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1AWTVMhqlgJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define a simple CNN architecture\n",
        "class FlModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FlModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(512 * 4 * 4, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = x.view(-1, 512 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize your model\n",
        "model = FlModel()\n",
        "\n",
        "# Optional: Initialize model weights if necessary\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        nn.init.zeros_(m.bias)\n",
        "\n",
        "model.apply(weights_init)\n",
        "\n",
        "# Save the initialized model\n",
        "torch.save(model.state_dict(), 'your_model.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pZAgxCacaPB"
      },
      "source": [
        "## **Splits the training and testing sets among the available clients**\n",
        "\n",
        "Decentralized learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oZDlE-Gq3jZ"
      },
      "outputs": [],
      "source": [
        "def client_data_config(train_data_tuple):\n",
        "    client_data = [() for _ in range(NUMOFCLIENTS)]\n",
        "    num_of_each_dataset = len(train_data_tuple[0]) // NUMOFCLIENTS\n",
        "\n",
        "    for i in range(NUMOFCLIENTS):\n",
        "        split_data_index = random.sample(range(len(train_data_tuple[0])), num_of_each_dataset)\n",
        "        new_x_train = torch.stack([train_data_tuple[0][k] for k in split_data_index])\n",
        "        new_y_train = torch.tensor([train_data_tuple[1][k] for k in split_data_index])\n",
        "\n",
        "        client_data[i] = (new_x_train, new_y_train)\n",
        "\n",
        "    return client_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef7tKEwscp2w"
      },
      "source": [
        "## **Implementation of Particle Swarm Optimization Algorithm**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-i3IiMndg18"
      },
      "source": [
        "This code defines a Particle Swarm Optimization (PSO) algorithm for optimizing neural network weights in a federated learning context. Each particle corresponds to a client's model and is initialized with a unique identifier, the client model, and training data. The PSO algorithm updates the model's weights based on velocities, local best, and global best weights. The model is trained with the updated weights, and the best weights are saved. If the training loss improves, the local best model is updated. The global best model is updated if the local best score is lower than the global best score. The code allows retrieval of the best model based on particle ID. This PSO-based approach aims to optimize model performance across clients by dynamically adjusting model weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ClPRjXBrIOl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import random\n",
        "import copy\n",
        "\n",
        "class Particle:\n",
        "    def __init__(self, particle_num, x_train, y_train):\n",
        "        self.particle_id = particle_num\n",
        "        self.particle_model = FlModel().to(device)\n",
        "        self.local_best_model = FlModel().to(device)\n",
        "        self.global_best_model = FlModel().to(device)\n",
        "        self.local_best_score = float('inf')  # Initialize with positive infinity\n",
        "        self.global_best_score = float('inf')  # Initialize with positive infinity\n",
        "\n",
        "        self.x = x_train.to(device)\n",
        "        self.y = y_train.to(device)\n",
        "\n",
        "        self.parm = {'acc': 1.0, 'local_acc': 1.0, 'global_acc': 1.0}\n",
        "\n",
        "        # Initialize velocities with random values\n",
        "        self.velocities = [torch.randn_like(param) / 5 - 0.10 for param in self.particle_model.parameters()]\n",
        "\n",
        "    def train_particle(self):\n",
        "        print(\"particle {}/{} fitting\".format(self.particle_id + 1, NUMOFCLIENTS))\n",
        "\n",
        "        step_model = copy.deepcopy(self.particle_model)\n",
        "        step_model.train()\n",
        "\n",
        "        new_parameters = []\n",
        "        for i, (new_param, param, lb_param, gb_param) in enumerate(zip(self.particle_model.parameters(), step_model.parameters(), self.local_best_model.parameters(), self.global_best_model.parameters())):\n",
        "            new_v = self.parm['acc'] * self.velocities[i]\n",
        "            new_v += self.parm['local_acc'] * random.random() * (lb_param - param)\n",
        "            new_v += self.parm['global_acc'] * random.random() * (gb_param - param)\n",
        "            self.velocities[i] = new_v\n",
        "            new_parameters.append(param + self.velocities[i])\n",
        "\n",
        "        # Extend new_parameters to match the number of parameters in the model\n",
        "        while len(new_parameters) < len(list(step_model.parameters())):\n",
        "            new_parameters.append(torch.zeros_like(new_parameters[0]))\n",
        "\n",
        "        # Apply the new parameters to the model\n",
        "        with torch.no_grad():\n",
        "            for i, param in enumerate(step_model.parameters()):\n",
        "                param.data = new_parameters[i].data\n",
        "\n",
        "        optimizer = OPTIMIZER(step_model.parameters(), lr=lr)\n",
        "        criterion = LOSS\n",
        "\n",
        "        for epoch in range(CLIENT_EPOCHS):\n",
        "            for inputs, labels in DataLoader(TensorDataset(self.x, self.y), batch_size=BATCH_SIZE, shuffle=True):\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = step_model(inputs)\n",
        "\n",
        "                # Ensure the size of outputs and labels match\n",
        "                outputs = outputs[:labels.size(0), :]\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            step_model.eval()\n",
        "            outputs = step_model(self.x)\n",
        "            loss = criterion(outputs, self.y)\n",
        "\n",
        "        step_model.train()\n",
        "\n",
        "        if self.global_best_score >= loss:\n",
        "            self.local_best_model.load_state_dict(step_model.state_dict())\n",
        "            self.local_best_score = loss.item()\n",
        "\n",
        "        return self.particle_id, loss.item()\n",
        "\n",
        "    def update_global_model(self, global_best_model, global_best_score):\n",
        "        if self.local_best_score <= global_best_score:\n",
        "            self.global_best_model.load_state_dict(global_best_model.state_dict())\n",
        "            self.global_best_score = global_best_score\n",
        "\n",
        "    def resp_best_model(self, gid):\n",
        "        if self.particle_id == gid:\n",
        "            return copy.deepcopy(self.particle_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kChbvGQ3zmCw"
      },
      "outputs": [],
      "source": [
        "def get_best_score_by_loss(step_result):\n",
        "    # step_result = [[step_model, train_socre_acc],...]\n",
        "    temp_score = 100000\n",
        "    temp_index = 0\n",
        "\n",
        "    for index, result in enumerate(step_result):\n",
        "        if temp_score > result[1]:\n",
        "            temp_score = result[1]\n",
        "            temp_index = index\n",
        "\n",
        "    return step_result[temp_index][0], step_result[temp_index][1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mH0fjJh50Ffa"
      },
      "outputs": [],
      "source": [
        "def get_best_score_by_acc(step_result):\n",
        "    # step_result = [[step_model, train_socre_acc],...]\n",
        "    temp_score = 0\n",
        "    temp_index = 0\n",
        "\n",
        "    for index, result in enumerate(step_result):\n",
        "        if temp_score < result[1]:\n",
        "            temp_score = result[1]\n",
        "            temp_index = index\n",
        "\n",
        "    return step_result[temp_index][0], step_result[temp_index][1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veoczqF_D7jV"
      },
      "source": [
        "instantiate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEh6lvfLD7sO"
      },
      "outputs": [],
      "source": [
        "def init_model():\n",
        "    model = FlModel()\n",
        "    optimizer = OPTIMIZER(model.parameters(), lr=lr)\n",
        "    criterion = LOSS\n",
        "    return model, optimizer, criterion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJEZVvPxPp0e"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(model, data_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Get predicted labels\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Update counts\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5JjwrmePnYS"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k12G4ZlKPnhP"
      },
      "outputs": [],
      "source": [
        "def client_update(index, client, now_epoch, avg_state_dict, train_loader):\n",
        "    print(f\"client {index + 1}/{len(selected_model)} fitting\")\n",
        "\n",
        "    if now_epoch != 0:\n",
        "        client.load_state_dict(avg_state_dict)\n",
        "\n",
        "    # Define the optimizer and other necessary settings\n",
        "    optimizer = OPTIMIZER(client.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(CLIENT_EPOCHS):\n",
        "        # Your training loop here\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = client(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print loss for each iteration\n",
        "            # print(f\"Client {index + 1}/{len(selected_model)} - Epoch {now_epoch + 1}/{EPOCHS} - Loss: {loss.item()}\")\n",
        "\n",
        "            accuracy = calculate_accuracy(client, train_loader)\n",
        "            client.accuracy = accuracy  # Store accuracy as an attribute\n",
        "            print(f\"Client {index + 1}/{int(NUMOFCLIENTS * SELECT_CLIENTS)} - Epoch {epoch + 1}/{CLIENT_EPOCHS} - Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    return client.state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyjP1nM3cgjK"
      },
      "source": [
        "## **Runs federated learning code on client datasets, displaying epochs, weighted average, loss, and accuracy.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iRWVftI0H8-",
        "outputId": "af941828-8112-4e55-924b-74a2e311a261"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "particle 1/10 fitting\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    train_loader, test_loader = load_dataset()\n",
        "\n",
        "    # Pass the tuple to client_data_config\n",
        "    client_data = client_data_config(train_loader)\n",
        "\n",
        "    # Extract the dataset from the tuple\n",
        "    train_dataset, _ = train_loader\n",
        "\n",
        "    pso_model = [Particle(particle_num=i, x_train=client_data[i][0], y_train=client_data[i][1]) for i in range(NUMOFCLIENTS)]\n",
        "\n",
        "    server_evaluate_acc = []\n",
        "    global_best_model = None\n",
        "    global_best_score = 0.0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        server_result = []\n",
        "        start = time.time()\n",
        "\n",
        "        for particle in pso_model:\n",
        "            if epoch != 0:\n",
        "                particle.update_global_model(server_model, global_best_score)\n",
        "\n",
        "            pid, train_score = particle.train_particle()\n",
        "            rand = random.randint(0, 99)\n",
        "\n",
        "            # Randomly dropped data sent to the server\n",
        "            drop_communication = range(DROP_RATE)\n",
        "            if rand not in drop_communication:\n",
        "                server_result.append([pid, train_score])\n",
        "\n",
        "        # Send the optimal model to each particle after the best score comparison\n",
        "        gid, global_best_score = get_best_score_by_loss(server_result)\n",
        "        for particle in pso_model:\n",
        "            if particle.resp_best_model(gid) is not None:\n",
        "                global_best_model = particle.resp_best_model(gid)\n",
        "\n",
        "        server_model.load_state_dict(global_best_model.state_dict())\n",
        "\n",
        "        print(\"server {}/{} evaluate\".format(epoch + 1, EPOCHS))\n",
        "        accuracy = calculate_accuracy(server_model, test_loader)\n",
        "        print(f\"Epoch {epoch + 1}/{EPOCHS} - Accuracy on test set: {accuracy}\")\n",
        "        server_evaluate_acc.append(accuracy)\n",
        "\n",
        "    write_csv(\"FedPSO\", server_evaluate_acc)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}